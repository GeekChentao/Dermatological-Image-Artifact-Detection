{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "667ffaa2",
   "metadata": {},
   "source": [
    "# Automatic concept annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import clip\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from PIL import Image\n",
    "import scipy\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e314507c",
   "metadata": {},
   "source": [
    "# Utils funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28157286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading images and associated metadata.\n",
    "\n",
    "    Args:\n",
    "        image_path_list (list): A list of file paths to the images.\n",
    "        transform (callable): A function/transform to apply to the images.\n",
    "        metadata_df (pandas.DataFrame, optional): A pandas DataFrame containing metadata for the images.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the length of `image_path_list` is not equal to the length of `metadata_df`.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the image and metadata (if available) for a given index.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_path_list, transform, metadata_df=None):\n",
    "        self.image_path_list = image_path_list\n",
    "        self.transform = transform\n",
    "        self.metadata_df = metadata_df\n",
    "\n",
    "        if self.metadata_df is None:\n",
    "            self.metadata_df = pd.Series(index=self.image_path_list)\n",
    "        else:\n",
    "            assert len(self.image_path_list) == len(\n",
    "                self.metadata_df\n",
    "            ), \"image_path_list and metadata_df must have the same length\"\n",
    "            self.metadata_df.index = self.image_path_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_path_list[idx])\n",
    "\n",
    "        ret = {\"path\": str(self.image_path_list[idx]), \"image\": self.transform(image)}\n",
    "\n",
    "        if self.metadata_df is not None:\n",
    "            ret.update({\"metadata\": self.metadata_df.iloc[idx]})\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6634103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    \"\"\"Custom collate function for the dataloader.\n",
    "\n",
    "    Args:\n",
    "        batch (list): list of dictionaries, each dictionary is a batch of data\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of collated data\n",
    "    \"\"\"\n",
    "\n",
    "    ret = {}\n",
    "    for key in batch[0]:\n",
    "        if isinstance(batch[0][key], pd.Series):\n",
    "            try:\n",
    "                ret[key] = pd.concat([d[key] for d in batch], axis=1).T\n",
    "            except RuntimeError:\n",
    "                raise RuntimeError(f\"Error while concatenating {key}\")\n",
    "        else:\n",
    "            # print(f\"{key} at custom collate\")\n",
    "            try:\n",
    "                ret[key] = torch.utils.data.dataloader.default_collate(\n",
    "                    [d[key] for d in batch]\n",
    "                )\n",
    "            except RuntimeError:\n",
    "                raise RuntimeError(f\"Error while concatenating {key}\")\n",
    "    # print(ret)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def custom_collate_per_key(batch_all):\n",
    "    \"\"\"Custom collate function batched outputs.\n",
    "\n",
    "    Args:\n",
    "        batch_all (dict): dictionary of lists of objects, each dictionary is a batch of data\n",
    "    Returns:\n",
    "        dict: dictionary of collated data\n",
    "    \"\"\"\n",
    "\n",
    "    # print(batch_all.keys())\n",
    "    # print(batch_all[\"image_features\"][0].shape)\n",
    "    # print(batch_all[\"metadata\"][0])\n",
    "\n",
    "    ret = {}\n",
    "    for key in batch_all:\n",
    "        if isinstance(batch_all[key][0], pd.DataFrame):\n",
    "            # print(f\"key = {key}, which is DataFrame\")\n",
    "            ret[key] = pd.concat(batch_all[key], axis=0)\n",
    "        elif isinstance(batch_all[key][0], torch.Tensor):\n",
    "            # print(f\"key = {key}, which is Tensor\")\n",
    "            # print(batch_all[key][0].shape)\n",
    "            ret[key] = torch.concat(batch_all[key], axis=0)\n",
    "            # print(ret[key].shape)\n",
    "        else:\n",
    "            # print(f\"Collating {key}...\")\n",
    "            ret[key] = torch.utils.data.dataloader.default_collate(\n",
    "                [elem for batch in tqdm.tqdm(batch_all[key]) for elem in batch]\n",
    "            )\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def dataloader_apply_func(\n",
    "    dataloader, func, collate_fn=custom_collate_per_key, verbose=True\n",
    "):\n",
    "    \"\"\"Apply a function to a dataloader.\n",
    "\n",
    "    Args:\n",
    "        dataloader (torch.utils.data.DataLoader): torch dataloader\n",
    "        func (function): function to apply to each batch\n",
    "        collate_fn (function, optional): collate function. Defaults to custom_collate_batch.\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of outputs\n",
    "    \"\"\"\n",
    "    func_out_dict = {}\n",
    "\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        for key, func_out in func(batch).items():\n",
    "            func_out_dict.setdefault(key, []).append(func_out)\n",
    "\n",
    "    return collate_fn(func_out_dict)\n",
    "\n",
    "\n",
    "def convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "\n",
    "def get_transform(n_px):\n",
    "\n",
    "    return T.Compose(\n",
    "        [\n",
    "            T.Resize(n_px, interpolation=T.InterpolationMode.BICUBIC),\n",
    "            T.CenterCrop(n_px),\n",
    "            convert_image_to_rgb,\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1fe1a",
   "metadata": {},
   "source": [
    "# Model initilaize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d26858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the GPU device to use\n",
    "device = \"cuda:0\"\n",
    "device = \"cpu\"\n",
    "\n",
    "model_api = \"clip\"\n",
    "\n",
    "if model_api == \"clip\":\n",
    "    # Load model using original clip implementation\n",
    "    model, preprocess = clip.load(\"ViT-L/14\", device=device, jit=False)[\n",
    "        0\n",
    "    ], get_transform(n_px=224)\n",
    "    model.load_state_dict(\n",
    "        torch.hub.load_state_dict_from_url(\n",
    "            \"https://aimslab.cs.washington.edu/MONET/weight_clip.pt\",\n",
    "            map_location=torch.device(\"cpu\"),\n",
    "        )\n",
    "    )\n",
    "    model.eval()\n",
    "    print(\"model was loaded using original clip implementation\")\n",
    "else:\n",
    "    # Load model using huggingface clip implementation\n",
    "    processor_hf = AutoProcessor.from_pretrained(\"chanwkim/monet\")\n",
    "    model_hf = AutoModelForZeroShotImageClassification.from_pretrained(\"chanwkim/monet\")\n",
    "    model_hf.to(device)\n",
    "    model_hf.eval()\n",
    "    print(\"model was loaded using huggingface clip implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa79667",
   "metadata": {},
   "source": [
    "## Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87d1ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "annotated = True\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "image_dir = os.path.join(data_dir, \"Fitzpatric_subset\")\n",
    "if annotated:\n",
    "    data_csv = os.path.join(data_dir, \"annotated_data.csv\")\n",
    "else:\n",
    "    data_csv = os.path.join(\n",
    "        data_dir, \"data.csv\"\n",
    "    )  # Load annotated data, the score for each concept is available, the score is generated by MONET\n",
    "\n",
    "data = pd.read_csv(data_csv)\n",
    "suffix = \".jpg\"\n",
    "image_path_list = data[\"image_path\"]\n",
    "\n",
    "image_path_list = [\n",
    "    os.path.join(image_dir, f\"{path}{suffix}\") for path in image_path_list\n",
    "]\n",
    "data[\"path\"] = image_path_list\n",
    "\n",
    "print(f\"total number of images = {len(image_path_list)}\")\n",
    "data.head()\n",
    "\n",
    "image_dataset = ImageDataset(\n",
    "    image_path_list[\n",
    "        :20\n",
    "    ],  # pick 20 images for demo only, don't slice if you want to sort and plot entire dataset\n",
    "    preprocess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee2e25b",
   "metadata": {},
   "source": [
    "## Get image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380147b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to let MONET generate the concept scores, otherwise skip this entire section\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    image_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=0,\n",
    "    collate_fn=custom_collate,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "def batch_func(batch):\n",
    "    with torch.no_grad():\n",
    "        if model_api == \"clip\":\n",
    "            image_features = model.encode_image(batch[\"image\"].to(device))\n",
    "        else:\n",
    "            image_features = model_hf.get_image_features(batch[\"image\"].to(device))\n",
    "\n",
    "    # print(\"path:\")\n",
    "    # print(batch[\"path\"])\n",
    "    return {\n",
    "        \"image_features\": image_features.detach().cpu(),\n",
    "        \"metadata\": batch[\"metadata\"],\n",
    "    }\n",
    "\n",
    "\n",
    "image_embedding = dataloader_apply_func(\n",
    "    dataloader=dataloader,\n",
    "    func=batch_func,\n",
    "    collate_fn=custom_collate_per_key,\n",
    ")\n",
    "\n",
    "print(f\"embedding shape = {image_embedding[\"image_features\"].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e491fd18",
   "metadata": {},
   "source": [
    "## Get concept embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_embedding(\n",
    "    concept_term_list=[],\n",
    "    prompt_template_list=[\n",
    "        \"This is skin image of {}\",\n",
    "        \"This is dermatology image of {}\",\n",
    "        \"This is image of {}\",\n",
    "    ],\n",
    "    prompt_ref_list=[\n",
    "        [\"This is skin image\"],\n",
    "        [\"This is dermatology image\"],\n",
    "        [\"This is image\"],\n",
    "    ],\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate prompt embeddings for a concept\n",
    "\n",
    "    Args:\n",
    "        concept_term_list (list): List of concept terms that will be used to generate prompt target embeddings.\n",
    "        prompt_template_list (list): List of prompt templates.\n",
    "        prompt_ref_list (list): List of reference phrases.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the normalized prompt target embeddings and prompt reference embeddings.\n",
    "    \"\"\"\n",
    "    # target embedding\n",
    "    prompt_target = [\n",
    "        [prompt_template.format(term) for term in concept_term_list]\n",
    "        for prompt_template in prompt_template_list\n",
    "    ]  # [3, n] where n = num_concept\n",
    "    # print(prompt_target)\n",
    "    prompt_target_tokenized = [\n",
    "        clip.tokenize(prompt_list, truncate=True) for prompt_list in prompt_target\n",
    "    ]\n",
    "    # print(prompt_target_tokenized)\n",
    "    # print(prompt_target_tokenized[0].shape)\n",
    "    with torch.no_grad():\n",
    "        prompt_target_embedding = torch.stack(\n",
    "            [\n",
    "                model.encode_text(prompt_tokenized.to(next(model.parameters()).device))\n",
    "                .detach()\n",
    "                .cpu()\n",
    "                # model_hf.get_text_features(prompt_tokenized.to(next(model.parameters()).device)).detach().cpu()\n",
    "                for prompt_tokenized in prompt_target_tokenized\n",
    "            ]\n",
    "        )  # [3, 77]\n",
    "    # print(prompt_target_embedding.shape)\n",
    "    prompt_target_embedding_norm = (\n",
    "        prompt_target_embedding / prompt_target_embedding.norm(dim=2, keepdim=True)\n",
    "    )\n",
    "\n",
    "    # reference embedding\n",
    "    prompt_ref_tokenized = [\n",
    "        clip.tokenize(prompt_list, truncate=True) for prompt_list in prompt_ref_list\n",
    "    ]\n",
    "    with torch.no_grad():\n",
    "        prompt_ref_embedding = torch.stack(\n",
    "            [\n",
    "                model.encode_text(prompt_tokenized.to(next(model.parameters()).device))\n",
    "                .detach()\n",
    "                .cpu()\n",
    "                # model_hf.get_text_features(prompt_tokenized.to(next(model.parameters()).device)).detach().cpu()\n",
    "                for prompt_tokenized in prompt_ref_tokenized\n",
    "            ]\n",
    "        )\n",
    "    prompt_ref_embedding_norm = prompt_ref_embedding / prompt_ref_embedding.norm(\n",
    "        dim=2, keepdim=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"prompt_target_embedding_norm\": prompt_target_embedding_norm,\n",
    "        \"prompt_ref_embedding_norm\": prompt_ref_embedding_norm,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd768b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the concept \"bullae\", we here use the terms \"bullae\" and \"blister\" to generate the prompt embedding.\n",
    "artifacts = [\n",
    "    \"pen\",\n",
    "    \"hair strands\",\n",
    "    \"nail\",\n",
    "    \"finger\",\n",
    "    \"ear\",\n",
    "    \"eye\",\n",
    "    \"nostril\",\n",
    "    \"lip\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c432f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort all images base on the scores for each artifact and display if we have all the scores already, this can take sometime because of the large number of images.\n",
    "\n",
    "desc = True\n",
    "for artifact in artifacts[6:]:\n",
    "    scores = data[artifact].values\n",
    "    example_per_concept = len(scores)\n",
    "\n",
    "    fig = plt.figure(\n",
    "        figsize=(100 * 2, 1.3 * (example_per_concept // 100 + 1) * 2), facecolor=\"white\"\n",
    "    )\n",
    "\n",
    "    # Main GridSpec (num_artifacts row, 1 column)\n",
    "    main_gs = gridspec.GridSpec(1, 1, figure=fig)\n",
    "\n",
    "    # Nested GridSpec within the first subplot of main GridSpec\n",
    "    # Adjust rows based on examples per concept\n",
    "    nested_gs = gridspec.GridSpecFromSubplotSpec(\n",
    "        example_per_concept // 100\n",
    "        + (1 if example_per_concept % 100 > 0 else 0),  # rows\n",
    "        100,\n",
    "        subplot_spec=main_gs[0],\n",
    "        wspace=0,\n",
    "        hspace=0.1,\n",
    "        width_ratios=[1] * 100,  # left box narrower\n",
    "    )\n",
    "\n",
    "    # Dictionary to store axes for later use\n",
    "    axd = {}\n",
    "    for rank_num in range(example_per_concept):\n",
    "        ax = plt.Subplot(fig, nested_gs[rank_num])\n",
    "        fig.add_subplot(ax)\n",
    "\n",
    "        if desc:\n",
    "            path = image_path_list[np.argsort(scores)[::-1][rank_num]]\n",
    "        else:\n",
    "            path = image_path_list[np.argsort(scores)[rank_num]]\n",
    "\n",
    "        # Generate a simple pattern for demonstration\n",
    "        image = Image.open(path)\n",
    "        # Display the image\n",
    "        ax.imshow(preprocess.transforms[1](preprocess.transforms[0](image)))\n",
    "        ax.axis(\"off\")  # Remove axes for a cleaner look\n",
    "        ax.set_facecolor(\"white\")\n",
    "\n",
    "        # Example key, replace with your actual key\n",
    "        plot_key = rank_num\n",
    "        axd[plot_key] = ax\n",
    "\n",
    "        axd[plot_key].set_title(\n",
    "            f\"Rank: {rank_num}\\nScore: {scores[np.argsort(scores)[::-1][rank_num]]:.2f}\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\n",
    "        f\"{artifact} Scores {\"Desc\" if desc else \"Asc\"}\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "        y=1.005,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f63df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_embedding_dict = {}\n",
    "\n",
    "for artifact in artifacts:\n",
    "    concept_embedding_dict[artifact] = get_prompt_embedding(\n",
    "        concept_term_list=[artifact]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc6ad7",
   "metadata": {},
   "source": [
    "## Calculate concept presence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae99182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_concept_presence_score(\n",
    "    image_features_norm,\n",
    "    prompt_target_embedding_norm,\n",
    "    prompt_ref_embedding_norm,\n",
    "    temp=1 / np.exp(4.5944),\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates the concept presence score based on the given image features and concept embeddings.\n",
    "\n",
    "    Args:\n",
    "        image_features_norm (numpy.Tensor): Normalized image features.\n",
    "        prompt_target_embedding_norm (torch.Tensor): Normalized concept target embedding.\n",
    "        prompt_ref_embedding_norm (torch.Tensor): Normalized concept reference embedding.\n",
    "        temp (float, optional): Temperature parameter for softmax. Defaults to 1 / np.exp(4.5944).\n",
    "\n",
    "    Returns:\n",
    "        np.array: Concept presence score.\n",
    "    \"\"\"\n",
    "\n",
    "    target_similarity = (\n",
    "        prompt_target_embedding_norm.float() @ image_features_norm.T.float()\n",
    "    )\n",
    "    ref_similarity = prompt_ref_embedding_norm.float() @ image_features_norm.T.float()\n",
    "\n",
    "    target_similarity_mean = target_similarity.mean(dim=[1])\n",
    "    ref_similarity_mean = ref_similarity.mean(axis=1)\n",
    "\n",
    "    concept_presence_score = scipy.special.softmax(\n",
    "        [target_similarity_mean.numpy() / temp, ref_similarity_mean.numpy() / temp],\n",
    "        axis=0,\n",
    "    )[0, :].mean(axis=0)\n",
    "\n",
    "    return concept_presence_score\n",
    "\n",
    "\n",
    "image_features_norm = image_embedding[\"image_features\"] / image_embedding[\n",
    "    \"image_features\"\n",
    "].norm(dim=1, keepdim=True)\n",
    "\n",
    "concept_presence_score_dict = {}\n",
    "for artifact, concept_embedding in concept_embedding_dict.items():\n",
    "    concept_presence_score = calculate_concept_presence_score(\n",
    "        image_features_norm=image_features_norm,\n",
    "        prompt_target_embedding_norm=concept_embedding[\"prompt_target_embedding_norm\"],\n",
    "        prompt_ref_embedding_norm=concept_embedding[\"prompt_ref_embedding_norm\"],\n",
    "    )\n",
    "    concept_presence_score_dict[artifact] = concept_presence_score\n",
    "\n",
    "# data.to_csv(\"annotated_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fb25c3",
   "metadata": {},
   "source": [
    "## Plot top 10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b8c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_per_concept = 10\n",
    "\n",
    "num_artifacts = len(artifacts)\n",
    "\n",
    "# Create a figure\n",
    "fig = plt.figure(\n",
    "    figsize=(10 * 2, num_artifacts / 2 * (example_per_concept // 10 + 1) * 2)\n",
    ")\n",
    "\n",
    "# Main GridSpec (num_artifacts row, 1 column)\n",
    "main_gs = gridspec.GridSpec(num_artifacts, 1, figure=fig)\n",
    "\n",
    "\n",
    "for idx, (artifact, concept_presence_score) in enumerate(\n",
    "    concept_presence_score_dict.items()\n",
    "):\n",
    "    # Nested GridSpec within the first subplot of main GridSpec\n",
    "    # Adjust rows based on examples per concept\n",
    "    nested_gs = gridspec.GridSpecFromSubplotSpec(\n",
    "        example_per_concept // 10 + (1 if example_per_concept % 10 > 0 else 0),  # rows\n",
    "        10 + 1,\n",
    "        subplot_spec=main_gs[idx],\n",
    "        wspace=0,\n",
    "        hspace=0.1,\n",
    "        width_ratios=[1.5] + [1] * 10,  # left box narrower\n",
    "    )\n",
    "\n",
    "    label_ax = plt.Subplot(fig, nested_gs[0])\n",
    "    fig.add_subplot(label_ax)\n",
    "    label_ax.axis(\"off\")\n",
    "    label_ax.text(0.5, 0.5, artifact, fontsize=12, ha=\"center\", va=\"center\")\n",
    "\n",
    "    # Dictionary to store axes for later use\n",
    "    axd = {}\n",
    "    for rank_num in range(example_per_concept + 1)[1:]:\n",
    "        ax = plt.Subplot(fig, nested_gs[rank_num])\n",
    "        fig.add_subplot(ax)\n",
    "\n",
    "        path = image_path_list[np.argsort(concept_presence_score)[::-1][rank_num]]\n",
    "        # Generate a simple pattern for demonstration\n",
    "        image = Image.open(path)\n",
    "        # Display the image\n",
    "        ax.imshow(preprocess.transforms[1](preprocess.transforms[0](image)))\n",
    "        ax.axis(\"off\")  # Remove axes for a cleaner look\n",
    "\n",
    "        # Example key, replace with your actual key\n",
    "        plot_key = rank_num\n",
    "        axd[plot_key] = ax\n",
    "\n",
    "        axd[plot_key].set_title(\n",
    "            f\"Rank: {rank_num}\\nScore: {concept_presence_score[np.argsort(concept_presence_score)[::-1][rank_num]]:.2f}\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
